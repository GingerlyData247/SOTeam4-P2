#!/usr/bin/env python3
# SWE 45000, PIN FALL 2025 — TEAM 4 — PHASE 1
# DISCLAIMER: This file contains code either partially or entirely written by AI.

"""
Executable CLI 'run' for Phase 1.

Usage:
  ./run install        -> installs dependencies from requirements.txt
  ./run test           -> runs test suite and prints: "X/Y test cases passed. Z% line coverage achieved."
  ./run URL_FILE       -> processes newline-delimited URLs and prints NDJSON (one JSON object per model line)
"""

from __future__ import annotations

# ---------------------------- stdlib
import argparse
import importlib
import importlib.util
import io
import json
import os
import pkgutil
import shutil
import stat
import subprocess
import sys
import tempfile
from contextlib import redirect_stdout, redirect_stderr
from pathlib import Path
from typing import Any, Callable, Dict, List, Tuple
from urllib.parse import urlparse

# ---------------------------- environment hardening for autograder
REPO_ROOT = Path(__file__).resolve().parent
SRC_DIR = REPO_ROOT / "src"

# Ensure we can `import src...` even if the grader runs us from elsewhere
for p in (str(REPO_ROOT), str(SRC_DIR)):
    if p not in sys.path:
        sys.path.insert(0, p)

# Ensure child processes inherit import path too
_prev_pp = os.environ.get("PYTHONPATH", "")
pp_parts = [str(REPO_ROOT), str(SRC_DIR)]
if _prev_pp:
    pp_parts.append(_prev_pp)
os.environ["PYTHONPATH"] = os.pathsep.join(pp_parts)

# Keep HuggingFace / tqdm quiet if present
os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")
os.environ.setdefault("TQDM_DISABLE", "1")
# Prevent foreign pytest plugins in grader environments from breaking collection
os.environ.setdefault("PYTEST_DISABLE_PLUGIN_AUTOLOAD", "1")

# Lazy import logger (avoid side-effects if logging package is missing)
try:
    from src.utils.logging import logger  # type: ignore
except Exception:  # pragma: no cover
    import logging as _logging

    _logging.basicConfig(level=_logging.INFO, format="%(levelname)s: %(message)s")
    logger = _logging.getLogger("run")


# ---------------------------- helpers
def _run(cmd: List[str], *, capture: bool = False) -> subprocess.CompletedProcess:
    """Run a subprocess with our environment; never raise."""
    try:
        return subprocess.run(
            cmd,
            check=False,
            text=True,
            capture_output=capture,
            env=dict(os.environ),
        )
    except Exception as exc:  # pragma: no cover
        logger.error("Subprocess failed: %s", exc)
        # mimic CompletedProcess on failure
        cp = subprocess.CompletedProcess(cmd, returncode=1)
        return cp


def _have(mod: str) -> bool:
    try:
        return importlib.util.find_spec(mod) is not None
    except Exception:
        return False


def _pip_install(*args: str) -> int:
    """Install with the exact interpreter we’re running under."""
    cp = _run([sys.executable, "-m", "pip", *args])
    return cp.returncode


def ensure_test_deps() -> None:
    """Install runtime & test deps if they’re not available."""
    # runtime deps
    if Path("requirements.txt").exists():
        _pip_install("install", "--no-cache-dir", "-r", "requirements.txt")

    # dev/test deps (optional)
    if Path("requirements-dev.txt").exists():
        _pip_install("install", "--no-cache-dir", "-r", "requirements-dev.txt")

    # fallback if grader ignores requirements-dev.txt
    if not (_have("pytest") and _have("coverage")):
        _pip_install("install", "--no-cache-dir", "pytest", "pytest-cov", "pytest-mock", "coverage")


def remove_readonly(func, path, excinfo):
    """Error handler for shutil.rmtree that removes read-only permissions (Windows-safe)."""
    try:
        os.chmod(path, stat.S_IWRITE)
    except Exception:
        pass
    func(path)


# ---------------------------- install / test
def handle_install() -> int:
    rc = 0
    if Path("requirements.txt").exists():
        rc = _run([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"]).returncode
        if rc != 0:
            logger.error("Dependency installation failed (exit %d)", rc)
            return rc
    if Path("requirements-dev.txt").exists():
        _run([sys.executable, "-m", "pip", "install", "-r", "requirements-dev.txt"])
    return rc


def handle_test() -> int:
    """
    Print exactly one line:
      'X/Y test cases passed. Z% line coverage achieved.'
    and exit 0 (the grader parses that line).
    """
    # Make sure pytest/coverage exist in grader VM
    ensure_test_deps()

    # 1) Authoritative counts via JUnit XML
    with tempfile.NamedTemporaryFile(delete=False, suffix=".xml") as tf:
        junit_path = tf.name

    # Limit discovery to our test dir; keep it quiet
    cp = _run([sys.executable, "-m", "pytest", "-q", "tests", f"--junitxml={junit_path}"], capture=True)

    passed = total = 0
    try:
        import xml.etree.ElementTree as ET

        root = ET.parse(junit_path).getroot()
        # xml can be <testsuites> or <testsuite>
        node = root if root.tag == "testsuite" else root.find("testsuite")
        if node is not None:
            tests = int(node.attrib.get("tests", "0"))
            fails = int(node.attrib.get("failures", "0"))
            errors = int(node.attrib.get("errors", "0"))
            skipped = int(node.attrib.get("skipped", "0"))
            total = tests
            passed = tests - fails - errors - skipped
    except Exception:
        # extremely small fallback if XML is missing/broken
        out = (cp.stdout or "") + (cp.stderr or "")
        # pytest -q prints "." per passed test; crude but better than zero
        passed = out.count(".")
        total = max(passed, 0)

    # 2) Coverage percent (quiet) — scope to src/ if present, else whole repo
    cov_targets = ["src"] if (REPO_ROOT / "src").exists() else ["."]
    cov_pct = "0"
    if _have("coverage"):
        _run([sys.executable, "-m", "coverage", "erase"], capture=True)
        _run([sys.executable, "-m", "coverage", "run", "-m", "pytest", "-q", "tests"], capture=True)
        rep = _run([sys.executable, "-m", "coverage", "report", "-m", *cov_targets], capture=True)
        for ln in (rep.stdout or "").splitlines()[::-1]:
            parts = ln.split()
            if parts and parts[-1].endswith("%"):
                cov_pct = parts[-1].rstrip("%")
                break

    # 3) EXACTLY one stdout line
    print(f"{passed}/{total} test cases passed. {cov_pct}% line coverage achieved.")
    return 0


# ---------------------------- core logic
def classify_url(url: str) -> str:
    """Return one of: MODEL | DATASET | CODE."""
    if not isinstance(url, str):
        return "CODE"
    u = url.strip()
    if not u:
        return "CODE"

    p = urlparse(u)
    host = (p.netloc or "").lower()
    path = (p.path or "").lower().lstrip("/")

    if host.endswith("huggingface.co"):
        return "DATASET" if path.startswith("datasets/") else "MODEL"

    if host in {"github.com", "gitlab.com", "bitbucket.org"}:
        return "CODE"

    return "CODE"


def load_metrics() -> Dict[str, Callable[[Dict[str, Any]], Tuple[float, int]]]:
    """Import metric modules from src/metrics; skip or silence ones that fail/print."""
    metrics: Dict[str, Callable] = {}
    metrics_pkg = "src.metrics"
    try:
        package = importlib.import_module(metrics_pkg)
    except ModuleNotFoundError:
        logger.debug("metrics package not found; proceeding with none")
        return metrics

    for _, mod_name, is_pkg in pkgutil.iter_modules(package.__path__, package.__name__ + "."):
        if is_pkg:
            continue
        try:
            # silence top-level prints during import
            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                module = importlib.import_module(mod_name)
        except Exception as e:
            logger.debug("Skipping metric %s (import failed): %s", mod_name, e)
            continue
        func = getattr(module, "metric", None)
        if callable(func):
            metrics[mod_name.split(".")[-1]] = func
    return metrics


def compute_metrics_for_model(resource: Dict[str, Any]) -> Dict[str, Any]:
    """Compute all metrics for a single model resource."""
    metrics = load_metrics()

    # --- Handle the category metric separately
    final_category = "Model"
    category_latency = 0
    if "category" in metrics:
        category_func = metrics.pop("category")
        try:
            final_category, category_latency = category_func(resource)
        except Exception:
            final_category = "Model (Error)"

    out: Dict[str, Any] = {
        "name": resource.get("name", "unknown"),
        "category": final_category,
        "url": resource.get("url"),
        "category_latency": category_latency,
    }

    # --- Process numeric metrics
    results: Dict[str, Tuple[float, int]] = {}
    for name, func in metrics.items():
        try:
            score, latency = func(resource)
            score = float(max(0.0, min(1.0, score)))
        except Exception as e:
            logger.debug("Metric %s failed on %s: %s", name, resource.get("url"), e)
            score, latency = 0.0, 0
        results[name] = (score, latency)

    for name, (score, latency) in results.items():
        if name == "size_score":
            out[name] = {
                "raspberry_pi": score,
                "jetson_nano": score,
                "desktop_pc": score,
                "aws_server": score,
            }
        else:
            out[name] = score
        out[f"{name}_latency"] = latency

    net_score = sum(val for val, _ in results.values()) / max(1, len(results)) if results else 0.0
    net_latency = sum(lat for _, lat in results.items())
    out["net_score"] = round(net_score, 4)
    out["net_score_latency"] = net_latency
    return out


def process_url_file(path_str: str) -> int:
    """
    Read URL file where each line is:
      code_link, dataset_link, model_link
    - Only model_link lines generate NDJSON (one JSON object per line).
    - Preserves input order.
    """
    # lazy imports so 'run install' doesn't fail on missing heavy libs
    try:
        from src.utils.repo_cloner import clone_repo_to_temp
    except Exception:
        clone_repo_to_temp = None
    try:
        from src.utils.github_link_finder import find_github_url_from_hf
    except Exception:
        find_github_url_from_hf = None

    p = Path(path_str)
    if not p.exists():
        logger.error("URL file not found: %s", path_str)
        print(f"Error: URL file not found: {path_str}", file=sys.stderr)
        return 1

    import csv as _csv
    resources: List[Dict[str, Any]] = []

    with p.open("r", encoding="utf-8") as fh:
        reader = _csv.reader(fh)
        for row in reader:
            row = [part.strip() for part in row if part and part.strip()]
            code_url = dataset_url = model_url = None

            if len(row) == 1:
                url = row[0]
                cat = classify_url(url)
                if cat == "MODEL":
                    model_url = url
                elif cat == "DATASET":
                    dataset_url = url
                else:
                    code_url = url
            else:
                parts = (row + ["", "", ""])[:3]
                code_url, dataset_url, model_url = (parts[0] or None, parts[1] or None, parts[2] or None)

            # Build resource object
            name = None
            chosen = model_url or dataset_url or code_url
            if chosen:
                if "huggingface.co" in chosen:
                    name = chosen.split("huggingface.co/")[-1].rstrip("/")
                elif "github.com" in chosen:
                    name = "/".join(chosen.rstrip("/").split("/")[-2:])
                else:
                    name = chosen.rstrip("/").split("/")[-1]

            resources.append(
                {
                    "code_url": code_url,
                    "dataset_url": dataset_url,
                    "model_url": model_url,
                    "category": classify_url(model_url or dataset_url or code_url) if chosen else "CODE",
                    "name": name or "unknown",
                    "url": model_url or code_url or dataset_url,
                }
            )

    # Filter only MODEL lines
    models = [r for r in resources if r.get("model_url") and classify_url(r["model_url"]) == "MODEL"]

    # For each model, try to find a repo to clone (GitHub or HF->GitHub)
    for r in models:
        repo_to_clone = None
        model_url = r.get("model_url", "") or ""

        if "github.com" in model_url:
            repo_to_clone = model_url
        elif "huggingface.co" in model_url and find_github_url_from_hf:
            try:
                with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                    repo_to_clone = find_github_url_from_hf(r["name"])  # may be None
            except Exception as e:
                logger.debug("hf->github mapping failed for %s: %s", r["name"], e)
                repo_to_clone = None

        if repo_to_clone and clone_repo_to_temp:
            try:
                with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                    local_dir = clone_repo_to_temp(repo_to_clone)
                r["local_path"] = local_dir
                r["local_dir"] = local_dir
            except Exception as e:
                logger.debug("Failed to clone %s (%s): %s", repo_to_clone, r.get("url"), e)
                r["local_path"] = r["local_dir"] = None
        else:
            r["local_path"] = r["local_dir"] = None

    # Process sequentially to preserve order and keep NDJSON clean
    for r in models:
        try:
            with redirect_stdout(io.StringIO()), redirect_stderr(io.StringIO()):
                result = compute_metrics_for_model(r)
        except Exception:
            logger.exception("Failed computing metrics for %s", r.get("url"))
            result = {
                "name": r.get("name", "unknown"),
                "category": "MODEL",
                "url": r.get("model_url") or r.get("url"),
                "net_score": 0.0,
                "net_score_latency": 0,
            }

        sys.stdout.write(json.dumps(result, ensure_ascii=False, separators=(",", ":")) + "\n")
        sys.stdout.flush()

        # cleanup any cloned local directories
        local = r.get("local_path") or r.get("local_dir")
        if local:
            try:
                shutil.rmtree(local, onerror=remove_readonly)
            except Exception:
                logger.debug("Cleanup failed for %s", local)

    return 0


# ---------------------------- CLI
def main(argv: List[str] | None = None) -> int:
    parser = argparse.ArgumentParser(prog="run", description="Phase 1 CLI for trustworthy model reuse")
    parser.add_argument("arg", nargs="?", help="install | test | URL_FILE")
    args = parser.parse_args(argv)

    if args.arg is None:
        parser.print_help()
        return 1

    if args.arg == "install":
        return handle_install()
    if args.arg == "test":
        return handle_test()

    return process_url_file(args.arg)


if __name__ == "__main__":
    sys.exit(main())
