from __future__ import annotations

from typing import Optional, List, Dict, Any, Generic, TypeVar
from pydantic import BaseModel, Field
from pydantic.generics import GenericModel

T = TypeVar("T")


class ModelCreate(BaseModel):
    name: str = Field(..., examples=["google-bert/bert-base-uncased"])
    version: str = Field(..., examples=["1.0.0"])
    card: str = Field("", description="Raw/markdown card text")
    tags: List[str] = Field(default_factory=list)
    metadata: Optional[Dict[str, Any]] = None
    source_uri: Optional[str] = None


class ModelUpdate(BaseModel):
    description: Optional[str] = None
    tags: Optional[List[str]] = None


class ModelOut(BaseModel):
    id: str
    name: str
    version: str
    metadata: Dict[str, Any]


class Page(GenericModel, Generic[T]):
    items: List[T]
    next_cursor: Optional[str] = None


# ----------------------------------------------------------------------
# NEW: Rating models, matching OpenAPI ModelRating schema
# ----------------------------------------------------------------------

class SizeScore(BaseModel):
    """
    Nested object for size_score:

    required:
      - raspberry_pi
      - jetson_nano
      - desktop_pc
      - aws_server
    """
    raspberry_pi: float = Field(
        ...,
        description="Size score for Raspberry Pi class devices."
    )
    jetson_nano: float = Field(
        ...,
        description="Size score for Jetson Nano deployments."
    )
    desktop_pc: float = Field(
        ...,
        description="Size score for desktop deployments."
    )
    aws_server: float = Field(
        ...,
        description="Size score for cloud server deployments."
    )


class ModelRating(BaseModel):
    """
    Model rating summary generated by the evaluation service.

    This matches the OpenAPI ModelRating schema.
    """

    # required: name, category
    name: str = Field(
        ...,
        description="Human-friendly label for the evaluated model.",
    )
    category: str = Field(
        ...,
        description="Model category assigned during evaluation.",
    )

    # net_score
    net_score: float = Field(
        ...,
        description="Overall score synthesizing all metrics.",
    )
    net_score_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `net_score`.",
    )

    # ramp_up_time
    ramp_up_time: float = Field(
        ...,
        description="Ease-of-adoption rating for the model.",
    )
    ramp_up_time_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `ramp_up_time`.",
    )

    # bus_factor
    bus_factor: float = Field(
        ...,
        description="Team redundancy score for the upstream project.",
    )
    bus_factor_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `bus_factor`.",
    )

    # performance_claims
    performance_claims: float = Field(
        ...,
        description="Alignment between stated and observed performance.",
    )
    performance_claims_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `performance_claims`.",
    )

    # license
    license: float = Field(
        ...,
        description="Licensing suitability score.",
    )
    license_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `license`.",
    )

    # dataset_and_code_score
    dataset_and_code_score: float = Field(
        ...,
        description="Availability and quality of accompanying datasets and code.",
    )
    dataset_and_code_score_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `dataset_and_code_score`.",
    )

    # dataset_quality
    dataset_quality: float = Field(
        ...,
        description="Quality rating for associated datasets.",
    )
    dataset_quality_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `dataset_quality`.",
    )

    # code_quality
    code_quality: float = Field(
        ...,
        description="Quality rating for provided code artifacts.",
    )
    code_quality_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `code_quality`.",
    )

    # reproducibility
    reproducibility: float = Field(
        ...,
        description="Likelihood that reported results can be reproduced.",
    )
    reproducibility_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `reproducibility`.",
    )

    # reviewedness
    reviewedness: float = Field(
        ...,
        description="Measure of peer or community review coverage.",
    )
    reviewedness_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `reviewedness`.",
    )

    # tree_score
    tree_score: float = Field(
        ...,
        description="Supply-chain health score for model dependencies.",
    )
    tree_score_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `tree_score`.",
    )

    # size_score (nested object)
    size_score: SizeScore = Field(
        ...,
        description="Size suitability scores for common deployment targets.",
    )
    size_score_latency: float = Field(
        ...,
        description="Time (seconds) required to compute `size_score`.",
    )


# Required in some Pydantic v2 setups when using __future__.annotations + generics.
ModelCreate.model_rebuild()
ModelUpdate.model_rebuild()
ModelOut.model_rebuild()
Page.model_rebuild()
SizeScore.model_rebuild()
ModelRating.model_rebuild()
